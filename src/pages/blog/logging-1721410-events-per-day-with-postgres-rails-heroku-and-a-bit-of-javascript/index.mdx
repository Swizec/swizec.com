---
title: "Logging 1,721,410 events per day with Postgres, Rails, Heroku, and a bit of JavaScript"
description: ""
published: 2018-12-17
redirect_from:
  - /blog/logging-events-postgres-rails-heroku-javascript/swizec/8825
categories: "Back End, Front End, Technical"
hero: ./img/techletter.app-screenshot-1545070245704.png
---

https://twitter.com/Swizec/status/1049786180483477504

When I was a wee lad of 21 or 23 I built my first startup. Designed for high scalability, we used Google App Engine for infinite horizontal scaling, Big Table to hold our oodles of data, many queues and sharding databases and tiny function objects that run independently on an infinite number of machines to deal with our massive scale.

A true engineering marvel, everyone said wow ðŸ‘Œ

Early NoSQL days, nascent serverless. Forget bare metal machines and servers in the cloud. We were too tough for that! Internet scale, baby!

After months, _months_, of engineering, the system was ready. It would scale beautifully and turn everybody's RSS feeds into algorithmic timelines based on each user's individual behavior. I was so proud.

Even gave a lecture about this system we built at my databases class in college. Professor liked it so much he added _my_ lecture to the final exam. Even became part of his normal curriculum the next year. ðŸ’ª

Moment of truth.

We flipped the switch. The system buzzed. The system whirred. The system melted our credit card, fell on its face, and got stuck.

Nothing worked.

One queue tried to process 5000 items per minute, pushing them into a queue that could handle 5 per hour. The database, so beautifully designed for scalability, broke down after 100 elements. Relations are hard in a key:value store. Indexing even harder.

10 users caused a complete meltdown of our infrastructure. 3000 articles to be scraped and processed every hour and our system just could not deal.

The startup never recovered.

## You're scaling too early

Fast forward a few years and I'm working at a startup that's actually successful. Not internet scale successful. Making real money from real users kind of successful. You know, a business.

We run a Ruby on Rails monolith on Heroku, using Postgres for our database, Redis for the Sidekiq job queue, JavaScript on the frontend. We think of Rails as an API server, SPA (single page app) bootstrapper, and worker runner. CloudFront and S3 host our static files.

We serve around 10 requests per second, store a few million rows in Postgres every day, keep a few hundred gigs of data, and process thousands of background jobs.

That runs on 6 Heroku web instances, 2 worker instances, and 1 medium-ish tier database server. The web dynos run at a chill 50% utilization, the workers sweat a bit. Postgres sometimes struggles with too many connections. Large queries might run out of memory if you aren't careful.

Heroku offers many higher tiers of database, but we try to avoid. Constraints are good for engineering.

So how is it that this absolutely boring stack can outperform my engineering marvel by 100x?

## A true approach to scale

> You don't know where it hurts until the system tells you.

That's my big lesson from the past 10 years of engineering. You don't know where something's gonna break until you push it and see what happens.

You just can't know. You can guess the basics.

Like, don't use triple nested loops to process 10,000,000 rows of data. Use database indexes for common queries. Use caching to avoid